{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, Imatrix, Rmatrix):\n",
    "        super(Controller, self).__init__()\n",
    "        self.bias = torch.ones(Rmatrix.shape[0])\n",
    "        self.Imatrix = Imatrix\n",
    "        self.Rmatrix = Rmatrix\n",
    "        self.register_buffer('prev_x', torch.zeros(Rmatrix.shape[0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_x = self.Imatrix @ x + self.Rmatrix @ self.prev_x + self.bias\n",
    "        return torch.argmax(self.prev_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, game, num_interactions, initial_variance):\n",
    "        # TODO: organize this shit\n",
    "        self.game = game\n",
    "        self.env = gym.make(game, obs_type=\"rgb\", frameskip=5)\n",
    "        self.search_dimensionality = self.env.action_space.n + self.env.action_space.n ** 2\n",
    "        self.search_u = torch.zeros(self.search_dimensionality) \n",
    "        self.search_Sigma = initial_variance * torch.eye(self.search_dimensionality)\n",
    "        self.search_A = torch.linalg.cholesky(self.search_Sigma)\n",
    "        self.sigma = torch.pow(torch.abs(torch.linalg.det(self.search_A)), 1 / self.search_dimensionality)\n",
    "        self.B = self.search_A / self.sigma\n",
    "        self.normal_distribution = torch.distributions.MultivariateNormal(torch.zeros(self.search_dimensionality), torch.eye(self.search_dimensionality))\n",
    "        self.population_size = int(1.5 * (4 + int(3 * np.log(self.search_dimensionality))))\n",
    "        self.num_interactions = num_interactions\n",
    "        self.lr_u = 0.5\n",
    "        self.lr_s = (3 / 5) * ((3 + np.log(self.search_dimensionality)) / self.search_dimensionality * np.sqrt(self.search_dimensionality))\n",
    "        self.lr_b = self.lr_s\n",
    "        self.env.close()\n",
    "\n",
    "    def generate_controller_weights(self):\n",
    "        controller_weights = []\n",
    "        for k in range(self.population_size):\n",
    "            sk = self.normal_distribution.sample()\n",
    "            zk = self.search_u + self.sigma * self.B.T @ sk\n",
    "            controller_weights.append((sk, zk))\n",
    "        return controller_weights\n",
    "\n",
    "    def initialise_controllers(self, controller_weights):\n",
    "        controllers = []\n",
    "        for weighttuple in controller_weights:\n",
    "            weight = weighttuple[1]\n",
    "            Rmatrix = torch.reshape(weight[:self.env.action_space.n ** 2], (self.env.action_space.n, self.env.action_space.n))\n",
    "            Imatrix = torch.reshape(weight[self.env.action_space.n ** 2:], (self.env.action_space.n, -1))\n",
    "            controllers.append(Controller(Imatrix, Rmatrix))\n",
    "        return controllers\n",
    "\n",
    "    def run_episodes(self, controllers, compressor):\n",
    "        \"\"\"\n",
    "        TODO: split the function into smaller functions\n",
    "        \"\"\"\n",
    "        fitness = []\n",
    "        memory = torch.zeros((1, 80,70))\n",
    "        for controller in controllers:\n",
    "            env = gym.make(self.game, obs_type=\"rgb\", frameskip=5)\n",
    "            observation, info = env.reset()\n",
    "            cummulative_reward = 0\n",
    "\n",
    "            for i in range(self.num_interactions):\n",
    "                comp_observation = compressor.downsize_image(torch.tensor(observation))\n",
    "                encoded_observation = compressor.encode_observation(comp_observation)\n",
    "                action = controller(encoded_observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                cummulative_reward += reward\n",
    "\n",
    "                if not torch.any(memory):\n",
    "                    memory[0] = comp_observation\n",
    "                else:\n",
    "                    memory = torch.cat((memory, comp_observation), 0)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            env.close()\n",
    "            fitness.append(cummulative_reward)\n",
    "                    \n",
    "        return fitness, memory\n",
    "    \n",
    "    def update_search_distribution(self, controller_weights, fitness):\n",
    "        \"\"\"\n",
    "        TODO: split the function into smaller functions\n",
    "        \"\"\"\n",
    "        sorted_weights = sorted(zip(fitness, controller_weights), key=lambda item: (item[0], item[1][0][0].item()), reverse=False)\n",
    "        sorted_weights = [sk for _, (sk, _) in sorted_weights]\n",
    "        divisor = sum(np.max((0.0, np.log(self.population_size / 2 + 1) - np.log(j))) for j in range(1, self.population_size + 1))\n",
    "        utilities = [np.max((0.0, np.log(self.population_size / 2 + 1) - np.log(i))) / divisor for i in range(1, self.population_size + 1)]\n",
    "\n",
    "        # Gradients calculation\n",
    "        grad_d = torch.zeros(self.search_dimensionality)\n",
    "        grad_m = torch.zeros(self.search_dimensionality, self.search_dimensionality)\n",
    "\n",
    "        for i, sk in enumerate(sorted_weights):\n",
    "            uk = utilities[i]\n",
    "            grad_d += uk * sk\n",
    "            grad_m += uk * (sk.unsqueeze(1) @ sk.unsqueeze(0) - torch.eye(self.search_dimensionality))\n",
    "        \n",
    "        grad_s = torch.trace(grad_m) / self.search_dimensionality\n",
    "        grad_b = grad_m - grad_s * torch.eye(self.search_dimensionality)\n",
    "\n",
    "        # Parameters update\n",
    "        self.search_u += self.lr_u * self.sigma * self.B @ grad_d\n",
    "        self.sigma = self.sigma * torch.exp(self.lr_s / 2 * grad_s)\n",
    "        self.B = self.B @ torch.linalg.matrix_exp(self.lr_b / 2 * grad_b)\n",
    "        self.search_A = self.sigma * self.B\n",
    "        self.search_Sigma = self.search_A.T @ self.search_A\n",
    "\n",
    "    def rescale_search_distribution(self, compressor, variance_new_weights=1): # worked semi-fine on 1\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        no_RNNinputs = (self.search_dimensionality - self.env.action_space.n ** 2) / self.env.action_space.n\n",
    "        additional_inputs = compressor.dictionary.shape[0] - no_RNNinputs\n",
    "        if additional_inputs > 0:\n",
    "            # Update mu\n",
    "            self.search_u = torch.cat((self.search_u, torch.zeros(int(additional_inputs * self.env.action_space.n))))\n",
    "\n",
    "            # Extend Sigma with new rows and columns\n",
    "            new_dimensionality = self.search_u.shape[0]\n",
    "            extended_Sigma = torch.eye(new_dimensionality) * variance_new_weights\n",
    "            extended_Sigma[:self.search_Sigma.shape[0], :self.search_Sigma.shape[0]] = self.search_Sigma\n",
    "            self.search_Sigma = extended_Sigma\n",
    "            self.search_Sigma += 1e-4 * torch.eye(new_dimensionality)\n",
    "            \n",
    "            # Update consequent parameters\n",
    "            # TODO: make seperate function that is called both in init and here\n",
    "            self.search_dimensionality = new_dimensionality\n",
    "            self.search_A = torch.linalg.cholesky(self.search_Sigma)\n",
    "            self.sigma = torch.pow(torch.abs(torch.linalg.det(self.search_A)), 1 / self.search_dimensionality)\n",
    "            self.B = self.search_A / self.sigma\n",
    "            self.normal_distribution = torch.distributions.MultivariateNormal(torch.zeros(self.search_dimensionality), torch.eye(self.search_dimensionality))\n",
    "\n",
    "            # Update learning parameters\n",
    "            self.population_size = 4 + int(3 * np.log(self.search_dimensionality))\n",
    "            self.lr_s = (3 / 5) * ((3 + np.log(self.search_dimensionality)) / self.search_dimensionality * np.sqrt(self.search_dimensionality))\n",
    "            self.lr_b = self.lr_s\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compressor():\n",
    "    def __init__(self, epsilon, omega, residual_threshold):\n",
    "        self.input_pixels = 70 * 80                                 # This could be done cleaner\n",
    "        self.dictionary = torch.zeros((1, self.input_pixels))\n",
    "        self.epsilon = epsilon\n",
    "        self.omega = omega\n",
    "        self.threshold = self.input_pixels * residual_threshold\n",
    "\n",
    "    def downsize_image(self, observation):\n",
    "        comp_observation = observation.permute(2, 0, 1)\n",
    "        comp_observation = comp_observation.float().mean(0).unsqueeze(0).unsqueeze(0)\n",
    "        comp_observation = F.interpolate(comp_observation, (80, 70), mode='bilinear', align_corners=False)\n",
    "        return comp_observation.squeeze(0)\n",
    "\n",
    "    def encode_observation(self, observation):\n",
    "        \"\"\"\n",
    "        DRSC\n",
    "        \"\"\"\n",
    "        P = torch.flatten(observation) / 255\n",
    "        o = torch.zeros(self.dictionary.shape[0])\n",
    "        w = 0\n",
    "\n",
    "        while torch.sum(P) / 5600 > self.epsilon and w < self.omega:\n",
    "            S = torch.norm(self.dictionary - P.unsqueeze(0), dim=1)\n",
    "            msc = torch.argmin(S)\n",
    "            \n",
    "            o[msc] = 1\n",
    "            w += 1\n",
    "            P = P - self.dictionary[msc]\n",
    "            F.relu(P, inplace=True)\n",
    "        return o\n",
    "    \n",
    "    def update_dictionary(self, training_set):\n",
    "        \"\"\"\n",
    "        IDVQ\n",
    "        \"\"\"\n",
    "        for image in training_set:\n",
    "            P = torch.flatten(image) / 255\n",
    "\n",
    "            if not torch.any(self.dictionary):\n",
    "                self.dictionary = P.unsqueeze(0)\n",
    "            else:\n",
    "                o = self.encode_observation(image)\n",
    "                P_hat = self.dictionary.T @ o\n",
    "                R = P - P_hat\n",
    "                torch.nn.functional.relu(R, inplace=True)\n",
    "                if torch.sum(R) > self.threshold:\n",
    "                    self.dictionary = torch.cat((self.dictionary, R.unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, game, num_interactions,initial_variance, epsilon, omega, residual_threshold):\n",
    "        # TODO: template system for different games pre-settings\n",
    "        self.optimizer = Optimizer(game, num_interactions, initial_variance)\n",
    "        self.compressor = Compressor(epsilon, omega, residual_threshold)\n",
    "\n",
    "    def train_population(self):\n",
    "        print(\"Population size is: \", self.optimizer.population_size)\n",
    "        controller_weights = self.optimizer.generate_controller_weights()\n",
    "        controllers = self.optimizer.initialise_controllers(controller_weights)\n",
    "        fitness, memory = self.optimizer.run_episodes(controllers, self.compressor)\n",
    "        print(\"The average reward is: \", sum(fitness) / len(fitness))\n",
    "        self.optimizer.update_search_distribution(controller_weights, fitness)\n",
    "        self.compressor.update_dictionary(memory)\n",
    "        print(\"Dictionary size is: \", self.compressor.dictionary.shape[0])\n",
    "        self.optimizer.rescale_search_distribution(self.compressor)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- RUN 1 ---------\n",
      "Population size is:  22\n",
      "The average reward is:  73.86363636363636\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 2 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  3.125\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 3 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 4 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 5 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 6 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 7 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 8 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 9 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 10 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 11 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 12 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 13 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 14 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 15 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 16 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 17 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 18 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 19 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 20 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 21 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 22 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 23 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 24 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 25 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 26 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 27 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 28 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 29 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 30 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 31 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 32 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 33 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 34 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 35 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 36 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 37 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 38 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 39 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 40 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 41 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 42 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 43 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 44 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 45 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 46 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 47 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 48 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 49 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 50 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 51 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 52 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 53 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 54 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 55 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 56 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 57 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 58 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 59 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n",
      "--------- RUN 60 ---------\n",
      "Population size is:  16\n",
      "The average reward is:  0.0\n",
      "Dictionary size is:  4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(game=\"ALE/Qbert-v5\",\n",
    "                  num_interactions=200,\n",
    "                  initial_variance=1,\n",
    "                  epsilon=0.005,\n",
    "                  omega=3,\n",
    "                  residual_threshold=0.00499\n",
    "                 )\n",
    "for i in range(60):\n",
    "    print(f\"--------- RUN {i + 1} ---------\")\n",
    "    trainer.train_population()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cf0ce7fe35de9b078ffb73081b6f9a6f5e41a15fcb33c0f281bb868a1000966"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
